services:
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
      args:
        VITE_OLLAMA_MODEL_BUILD: ${VITE_OLLAMA_MODEL:-deepseek-r1:latest}
    ports:
      - "3000:3000"
    environment:
      - VITE_OLLAMA_URL=http://localhost:11434/api
    restart: unless-stopped
    depends_on:
      - ollama
    networks:
      - ollama-network

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - .:/app
      - ollama_data:/root/.ollama
    environment:
      OLLAMA_KEEP_ALIVE: 24h
      OLLAMA_HOST: 0.0.0.0
      OLLAMA_MODELS: /root/.ollama/models
      OLLAMA_MODEL_DEFAULT: ${VITE_OLLAMA_MODEL:-deepseek-r1:latest}
    entrypoint: ["/bin/bash", "/app/entrypoint.sh"]
    restart: unless-stopped
    networks:
      - ollama-network


 # docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama


  open-webui: # New service for Open WebUI
    image: ghcr.io/open-webui/open-webui:ollama
    container_name: open-webui
    ports:
      - "8080:8080" # Open WebUI will be accessible on port 8080
    volumes:
      - ollama_data:/root/.ollama # Mounts the Ollama data volume for model access
      - open_webui_data:/app/backend/data # Dedicated volume for Open WebUI's data (e.g., chat history, user settings)
    environment: # Added environment variable
      - WEBUI_HOST=0.0.0.0 # Ensures the application binds to all network interfaces
    restart: unless-stopped
    depends_on:
      - ollama # Ensures Ollama starts before Open WebUI
    networks:
      - ollama-network

networks:
  ollama-network:
    external: false

volumes:
  ollama_data: # Volume for Ollama models and data
  open_webui_data: # New volume for Open WebUI's backend data
